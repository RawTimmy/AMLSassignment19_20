{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"Datasets/celeba/labels.csv\", index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and preprocess(crop or resize) the images\n",
    "def resize_get_image(row_id, root=\"Datasets/celeba/img\"):\n",
    "    # Open Image\n",
    "    file_name = \"{}.jpg\".format(row_id)\n",
    "    file_path = os.path.join(root, file_name)\n",
    "    img_ori = Image.open(file_path)\n",
    "    \n",
    "    # Cropped Image Parameters(Assuming all faces are in the middle 64x128)\n",
    "    left, right = 57, 121\n",
    "    top, bot = 45, 173\n",
    "    img_resize = img_ori.crop((left, top, right, bot))\n",
    "    \n",
    "    return np.array(img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(img):\n",
    "    color_features = img.flatten()\n",
    "    img_grey = rgb2grey(img)\n",
    "    hog_feartures = hog(img_grey, block_norm=\"L2-Hys\", pixels_per_cell=(16,16),cells_per_block=(2,2))\n",
    "    flat_features = np.hstack(color_features)\n",
    "    return flat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done:  0\n",
      "Done:  1000\n",
      "Done:  2000\n",
      "Done:  3000\n",
      "Done:  4000\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the features\n",
    "def create_feature_matrix(label_dataframe):\n",
    "    feature_list = []\n",
    "    i = 0\n",
    "    for img_id in label_dataframe.index:\n",
    "        img = resize_get_image(img_id)\n",
    "        image_features = create_features(img)\n",
    "        feature_list.append(image_features)\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"Done: \", i)\n",
    "        i+=1\n",
    "    \n",
    "    feature_matrix = np.array(feature_list)\n",
    "    return feature_matrix\n",
    "\n",
    "feature_matrix = create_feature_matrix(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape is:  (5000, 24576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rawtimmy/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Matrix Shape is:  (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Matrix Shape is: \", feature_matrix.shape)\n",
    "\n",
    "ss = StandardScaler()\n",
    "people_stand = ss.fit_transform(feature_matrix)\n",
    "\n",
    "pca = PCA()\n",
    "people_pca = pca.fit_transform(people_stand)\n",
    "print(\"PCA Matrix Shape is: \", people_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset - Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1782\n",
       " 1    1718\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(people_pca)\n",
    "y = pd.Series(labels.gender.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=1234123)\n",
    "\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 5000)\n",
      "(3500,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b6f68bc1c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgender_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 451\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "x_new = np.array([[[2,4],[10,10]],[[2,3],[3,4]]])\n",
    "y_new = np.array([1,-1])\n",
    "svm = SVC(kernel='linear', probability=True, random_state=42)\n",
    "gender_model = svm.fit(x_new, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', probability=True, random_state=42)\n",
    "gender_model = svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy is: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='sgd', \n",
    "                    activation='relu', \n",
    "                    alpha=1e-4, \n",
    "                    hidden_layer_sizes=(50,50), \n",
    "                    random_state=1, \n",
    "                    max_iter=20, \n",
    "                    verbose=10, \n",
    "                    learning_rate_init=0.001)\n",
    "gender_mlp_model = mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender MLP Model Acc: \" + str(mlp.score(X_test, y_test)))\n",
    "print(\"No. Layers: \" + str(mlp.n_layers_))\n",
    "print(\"No. Iterations: \" + str(mlp.n_iter_))\n",
    "print(\"Loss: \" + str(mlp.loss_))\n",
    "print(\"Output Activation Function: \" + str(mlp.out_activation_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset - Smiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smile = pd.DataFrame(people_pca)\n",
    "y_smile = pd.Series(labels.smiling.values)\n",
    "X_smile_train, X_smile_test, y_smile_train, y_smile_test = train_test_split(X_smile, y_smile, test_size = .3, random_state=1234123)\n",
    "\n",
    "pd.Series(y_smile_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Smiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_smile = SVC(kernel='linear', probability=True, random_state=42)\n",
    "smile_model = svm_smile.fit(X_smile_train, y_smile_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_smile_pred = svm_smile.predict(X_smile_test)\n",
    "\n",
    "accuracy_smile = accuracy_score(y_smile_test, y_smile_pred)\n",
    "print(\"Model Accuracy is: \", accuracy_smile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - Smiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_smile = MLPClassifier(solver='sgd', \n",
    "                    activation='relu', \n",
    "                    alpha=1e-4, \n",
    "                    hidden_layer_sizes=(50,50), \n",
    "                    random_state=1, \n",
    "                    max_iter=20, \n",
    "                    verbose=10, \n",
    "                    learning_rate_init=0.001)\n",
    "smile_mlp_model = mlp_smile.fit(X_smile_train, y_smile_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smiling MLP Model Acc: \" + str(mlp_smile.score(X_smile_test, y_smile_test)))\n",
    "print(\"No. Layers: \" + str(mlp_smile.n_layers_))\n",
    "print(\"No. Iterations: \" + str(mlp_smile.n_iter_))\n",
    "print(\"Loss: \" + str(mlp_smile.loss_))\n",
    "print(\"Output Activation Function: \" + str(mlp_smile.out_activation_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_weights_and_biases():\n",
    "    n_hidden_1 = 2048\n",
    "    n_hidden_2 = 2048\n",
    "    \n",
    "    X = tf.placeholder(\"float\", [None, 68, 2])\n",
    "    Y = tf.placeholder(\"float\", [None, 2])\n",
    "    \n",
    "    img_flat = tf.contrib.layers.flatten(X)\n",
    "    \n",
    "    stddev = 0.01\n",
    "    \n",
    "    weights = {\n",
    "        'hidden_layer1': tf.Variable(tf.random_normal([68*2, n_hidden_1], stddev = stddev)),\n",
    "        'hidden_layer2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev = stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, 2], stddev=stddev))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'bias_layer1': tf.Variable(tf.random_normal([n_hidden_1], stddev = stddev)),\n",
    "        'bias_layer2': tf.Variable(tf.random_normal([n_hidden_2], stddev = stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([2], stddev = stddev))\n",
    "    }\n",
    "    \n",
    "    return weights, biases, X, Y, img_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mlp():\n",
    "    weights, biases, X, Y, img_flat = allocate_weights_and_biases()\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(img_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
    "    layer_1 = tf.math.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
    "    layer_2 = tf.math.sigmoid(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 500\n",
    "\n",
    "# display training accuracy every ..\n",
    "display_accuracy_step = 2\n",
    "    \n",
    "\n",
    "#\n",
    "X_diu = pd.DataFrame(people_pca)\n",
    "y_diu = pd.Series(labels.gender.values)\n",
    "training_images, test_images, training_labels, test_labels = train_test_split(X_diu, y_diu, test_size = .3, random_state=1234123)\n",
    "#\n",
    "# training_images, training_labels, test_images, test_labels = get_data()\n",
    "logits, X, Y = model_mlp()\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# define training graph operation\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# graph operation to initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
    "                                                               Y: training_labels})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: training_labels})))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        # run test accuracy operation ..\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: test_images, Y: test_labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def features_extraction_with_labels(root=\"Datasets/celeba\"):\n",
    "#     # Read labels from csv\n",
    "#     labels = pd.read_csv(root+\"labels.csv\", index_col=0, sep='\\t')\n",
    "    \n",
    "#     img_paths = [os.path.join(root, \"img\"+file_name) for file_name in os.listdir(root)]\n",
    "    \n",
    "#     if os.path.isdir(root+\"/img\"):\n",
    "#         img_features = []\n",
    "#         img_labels = []\n",
    "#         gender_labels_series = pd.Series(labels.gender.values)\n",
    "        \n",
    "#         for img_path in img_paths:\n",
    "#             file_no = img_path.split('/')[-1].split('.')[0]\n",
    "            \n",
    "#             img = image.img_to_array(image.load_img(img_path, \n",
    "#                                                     target_size=None, \n",
    "#                                                     interpolation='bicubic'))\n",
    "            \n",
    "#             features, _ = img_preprocessing(img)\n",
    "            \n",
    "#             if features is not None:\n",
    "#                 img_features.append(features) \n",
    "#                 img_labels.append(gender_labels_series[int(file_no)])\n",
    "    \n",
    "#     landmark_features = np.array(img_features)\n",
    "#     gender_labels_new = (np.array(img_labels) + 1) / 2\n",
    "    \n",
    "#     return landmark_featrues, gender_labels_new\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
