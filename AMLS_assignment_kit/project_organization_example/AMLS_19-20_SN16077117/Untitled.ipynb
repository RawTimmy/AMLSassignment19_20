{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"Datasets/cartoon_set/labels.csv\", index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_cartoon=\"Datasets/cartoon_set/img\"\n",
    "basedir_cartoon = \"Datasets/cartoon_set\"\n",
    "labels_filename = \"labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img():\n",
    "    all_imgs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    image_paths_cartoon = [os.path.join(images_dir_cartoon, l) for l in os.listdir(images_dir_cartoon)]\n",
    "    labels_file = open(os.path.join(basedir_cartoon, labels_filename), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    \n",
    "    face_shape_labels = {line.split('\\t')[-1].split('\\n')[0] : int(line.split('\\t')[2]) for line in lines[1:]}\n",
    "    \n",
    "    if os.path.isdir(images_dir_cartoon):\n",
    "        \n",
    "        for img_path in image_paths_cartoon:\n",
    "\n",
    "            file_name = img_path.split('.')[0].split('/')[-1]\n",
    "#             file_name = img_path.split('.')[1].split('/')[-1]\n",
    "            \n",
    "            img = image.img_to_array(image.load_img(img_path,\n",
    "                                                    target_size=(100,100),\n",
    "                                                    interpolation='bicubic'))\n",
    "            all_imgs.append(img)\n",
    "            all_labels.append(face_shape_labels[file_name+'.png'])\n",
    "    \n",
    "    return np.asarray(all_imgs, np.float32), np.asarray(all_labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_example = data.shape[0]\n",
    "arr = np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "data_s = data[arr]\n",
    "label_s = label[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "s = np.int(num_example * ratio)\n",
    "x_train = data_s[:s]\n",
    "y_train = label_s[:s]\n",
    "x_val = data_s[s:]\n",
    "y_val = label_s[s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255.\n",
    "x_val = x_val/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 100, 100, 3], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None,], name='y_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\"weight\", \n",
    "                                        [5,5,3,32], \n",
    "                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "    \n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable(\"weight\", \n",
    "                                        [5,5,32,64], \n",
    "                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "    \n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    with tf.variable_scope('layer5-conv3'):\n",
    "        conv3_weights = tf.get_variable(\"weight\", \n",
    "                                        [3,3,64,128], \n",
    "                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv3_biases = tf.get_variable(\"bias\", [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv3 = tf.nn.conv2d(pool2, conv3_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))\n",
    "    \n",
    "    with tf.name_scope(\"layer6-pool3\"):\n",
    "        pool3 = tf.nn.max_pool(relu3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    with tf.variable_scope('layer7-conv4'):\n",
    "        conv4_weights = tf.get_variable(\"weight\", \n",
    "                                        [3,3,128,128], \n",
    "                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv4_biases = tf.get_variable(\"bias\", [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv4 = tf.nn.conv2d(pool3, conv4_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_biases))\n",
    "    \n",
    "    with tf.name_scope(\"layer8-pool4\"):\n",
    "        pool4 = tf.nn.max_pool(relu4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "        nodes = 6*6*128\n",
    "        reshaped = tf.reshape(pool4, [-1,nodes])\n",
    "    \n",
    "    with tf.variable_scope('layer9-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", \n",
    "                                      [nodes,1024], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: \n",
    "            tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "            \n",
    "        fc1_biases = tf.get_variable(\"bias\", [1024], initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "    with tf.variable_scope('layer10-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", \n",
    "                                      [1024,512], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: \n",
    "            tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "            \n",
    "        fc2_biases = tf.get_variable(\"bias\", [512], initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, fc2_weights) + fc2_biases)\n",
    "        if train:\n",
    "            fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "    \n",
    "    with tf.variable_scope('layer11-fc3'):\n",
    "        fc3_weights = tf.get_variable(\"weight\", \n",
    "                                      [512,5], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: \n",
    "            tf.add_to_collection('losses', regularizer(fc3_weights))\n",
    "            \n",
    "        fc3_biases = tf.get_variable(\"bias\", [5], initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "#         logit = tf.nn.softmax(tf.matmul(fc2, fc3_weights) + fc3_biases)\n",
    "        logit = tf.matmul(fc2, fc3_weights) + fc3_biases\n",
    "        \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "logits = inference(x, False, regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 50\n",
    "# saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"==========epoch %d==========\"%epoch)\n",
    "    \n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _, err, ac = sess.run([train_op, loss, acc], feed_dict={x: x_train_a, y_: y_train_a})\n",
    "        train_loss += err\n",
    "        train_acc += ac\n",
    "        n_batch += 1\n",
    "    print(\"train loss: %f\" % (np.sum(train_loss) / n_batch))\n",
    "    print(\"train acc: %f\" % (np.sum(train_acc) / n_batch))\n",
    "    \n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss, acc], feed_dict={x: x_val_a, y_: y_val_a})\n",
    "        val_loss += err\n",
    "        val_acc += ac\n",
    "        n_batch += 1\n",
    "    print(\"validation loss: %f\" % (np.sum(val_loss) / n_batch))\n",
    "    print(\"validation acc: %f\" % (np.sum(val_acc) / n_batch))\n",
    "# saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
